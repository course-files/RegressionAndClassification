{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4baf68",
   "metadata": {},
   "source": [
    "# Classification using the k‑Nearest Neighbors (kNN) Algorithm\n",
    "\n",
    "|                  |                                                                                                                                                                                                     |\n",
    "|:-----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Course Codes** | BBT 4106, BCM 3104, and BFS 4102                                                                                                                                                                    |\n",
    "| **Course Names** | BBT 4106: Business Intelligence I (Week 10-12 of 13),<br/>BCM 3104: Business Intelligence and Data Analytics (Week 10-12 of 13) and<br/>BFS 4102: Advanced Business Data Analytics (Week 4-6 of 13) |\n",
    "| **Semester**     | April to July 2025                                                                                                                                                                                  |\n",
    "| **Lecturer**     | Allan Omondi                                                                                                                                                                                        |\n",
    "| **Contact**      | aomondi@strathmore.edu                                                                                                                                                                              |\n",
    "| **Note**         | The lecture contains both theory and practice. This notebook forms part of the practice. This is intended for educational purpose only.                                                             |\n",
    "\n",
    "**Business context**: A global retail company has set a strategic objective *to increase customer satisfaction to 8/10 by Q3 of the current financial year.* The lagging KPI from the customer perspective of the business' performance is the customer satisfaction rating whereas its leading KPI is the number of late deliveries. The business would like to predict whether an order will arrive late so that supply chain management and logistics teams can intervene early and avoid dissatisfied customers.\n",
    "\n",
    "**Dataset**: The **\"DataCo Smart Supply Chain\"** dataset by **Constante et al. (2019)** contains 180,519 orders. A description of each feature and the target is presented via [this link](https://github.com/course-files/RegressionAndClassification/blob/main/data/DataCoSupplyChainDataset_description.csv). The following table presents the chosen features and the target.\n",
    "\n",
    "| **Type**    | **Name**                        | **Description**                                                                                 |\n",
    "|:------------|---------------------------------|:------------------------------------------------------------------------------------------------|\n",
    "| **Feature** | `Days for shipping (real)`      | Actual shipping days of the purchased product                                                   |\n",
    "| **Feature** | `Days for shipment (scheduled)` | Days of scheduled delivery of the purchased product                                             |\n",
    "| **Feature** | `Order Item Quantity`           | Number of products per order                                                                    |\n",
    "| **Feature** | `Sales`                         | Value in sales                                                                                  |\n",
    "| **Feature** | `Order Profit Per Order`        | Profit made through the order                                                                   |\n",
    "| **Feature** | `Shipping Mode`                 | The following shipping modes are presented: Standard Class, First Class, Second Class, Same Day |\n",
    "| **Target**  | `Late_delivery_risk`            | A categorical variable that indicates if sending is late (1) or it is not late (0)              |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Import the necessary libraries",
   "id": "d9746cb74f89b96d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose**: This chunk imports all the necessary libraries for data analysis, machine learning, and visualization related to the k-Nearest Neighbors classifier.\n",
    "\n",
    "1. **Data Manipulation Libraries**\n",
    "    - `pandas as pd`: For loading the dataset, creating and managing DataFrames, data manipulation and analysis using DataFrames\n",
    "    - `numpy as np`: For numerical operations and array manipulations\n",
    "\n",
    "2. **Machine Learning (scikit-learn) Components**\n",
    "    - `train_test_split`: Splits data into training and testing sets\n",
    "    - `GridSearchCV`: For hyperparameter tuning using cross-validation\n",
    "    - `StandardScaler`: For feature scaling\n",
    "    - `KNeighborsClassifier`: For implementing the kNN algorithm\n",
    "    - `classification_report, confusion_matrix`: For model evaluation metrics\n",
    "\n",
    "3. **Statistical Analysis (SciPy)**\n",
    "    - `kurtosis`: Measures the \"tailedness\" of data distribution\n",
    "    - `skew`: Measures the asymmetry of data distribution\n",
    "\n",
    "4. **Visualization Libraries**\n",
    "    - `matplotlib.pyplot as plt`: For basic plotting functionality\n",
    "    - `seaborn as sns`: For enhanced statistical visualizations\n",
    "\n",
    "5. **Warnings Management**\n",
    "    - `warnings`: Controls warning messages\n",
    "    - `warnings.filterwarnings('ignore')`: Suppresses warning messages for cleaner output\n",
    "    - Used to suppress warnings that may arise during the execution of the code. Even though it is not necessary for the code to run, it helps in keeping the output clean and focused on the results."
   ],
   "id": "ef65c15413a54b5e"
  },
  {
   "cell_type": "code",
   "id": "495456f6",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b72074f",
   "metadata": {},
   "source": "## Step 2: Load the data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:** This chunk handles the loading of the supply chain dataset with specific column selection.\n",
    "\n",
    "1. **URL Configuration**\n",
    "    - A commented-out local path option is provided (`./data/DataCoSupplyChainDataset.csv`)\n",
    "    - Currently using a direct GitHub raw content URL for data access\n",
    "    - This ensures data accessibility without requiring local file downloads\n",
    "\n",
    "2. **Data Loading Parameters**\n",
    "    - Uses `pandas.read_csv()` with specific parameters:\n",
    "        - `usecols`: Loads only the columns specified in `use_cols` for memory efficiency\n",
    "        - `encoding='latin-1'`: Handles special characters in the dataset. Other encodings like 'utf-8' may not work if the dataset contains characters outside the standard ASCII range. 'latin-1' is a common encoding that supports Western European languages and is often used when dealing with CSV files that contain special characters such as accents or currency symbols; ñ and €\n",
    "        - `nrows=200000`: Limits the number of rows loaded to 200,000. This can be reduced or increased based on the available memory and the size of the dataset.\n",
    "    - Data is stored in the `supply_chain_data` DataFrame for further analysis\n",
    "    - This selective loading approach helps manage memory usage and focuses the analysis on the relevant features for the delivery risk prediction task.\n"
   ],
   "id": "f8a406ccae8ad6a"
  },
  {
   "cell_type": "code",
   "id": "f3496160",
   "metadata": {},
   "source": [
    "# If you are using Google Colab, uncomment the following lines to mount your Google Drive and load the new data from there.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# url = '/content/drive/My Drive/Colab Notebooks/data/DataCoSupplyChainDataset.csv'\n",
    "\n",
    "# url = './data/DataCoSupplyChainDataset.csv'\n",
    "url = 'https://raw.githubusercontent.com/course-files/RegressionAndClassification/main/data/DataCoSupplyChainDataset.csv'\n",
    "use_cols = ['Days for shipping (real)', 'Days for shipment (scheduled)',\n",
    "            'Order Item Quantity', 'Sales', 'Order Profit Per Order',\n",
    "            'Shipping Mode', 'Late_delivery_risk']\n",
    "supply_chain_data = pd.read_csv(url, usecols=use_cols, encoding='latin-1', nrows=200000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Initial Exploratory Data Analysis (EDA)",
   "id": "ceebab0366fbd4c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n*1* The number of observations and variables\")\n",
    "display(supply_chain_data.shape)\n",
    "\n",
    "print(\"\\n*2* The data types:\")\n",
    "display(supply_chain_data.info())\n",
    "\n",
    "print(\"\\n*3* The summary of the numeric columns:\")\n",
    "display(supply_chain_data.describe())\n",
    "\n",
    "print(\"\\n*4* The whole dataset:\")\n",
    "display(supply_chain_data)\n",
    "\n",
    "print(\"\\n*5* The first 5 rows in the dataset:\")\n",
    "display(supply_chain_data.head())\n",
    "\n",
    "print(\"\\n*6* Percentage distribution for each category\")\n",
    "print(\"\\nNumber of observations per class:\")\n",
    "print(\"Frequency counts:\\n\", supply_chain_data['Late_delivery_risk'].value_counts())\n",
    "print(\"\\nPercentages:\\n\", supply_chain_data['Late_delivery_risk'].value_counts(normalize=True) * 100, \"%\")"
   ],
   "id": "27b79c5e4d2f03ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Measures of Distribution",
   "id": "e8b22317c778a59c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Variance of numeric columns",
   "id": "bffb018c1bc159f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Selection of numeric columns**\n",
    "- The code selects columns with numeric data types (`int64` and `float64`) that can be subjected to mathematica or statistical functions.\n",
    "- This is done using `select_dtypes()` method of the DataFrame, which filters columns based on their data types."
   ],
   "id": "e3677d5894ddc9d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols = supply_chain_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(\"\\nVariance of the numeric columns:\")\n",
    "print(supply_chain_data[numeric_cols].var())"
   ],
   "id": "fdfb3127677725d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Standard deviation of numeric columns",
   "id": "4bda0d9a8e16c3cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nStandard deviation of the numeric columns:\")\n",
    "print(supply_chain_data[numeric_cols].std())"
   ],
   "id": "874a8ad502a4ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Kurtosis of numeric columns",
   "id": "e10bc14d078524a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nFisher Kurtosis of numeric columns:\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"→ Positive kurtosis indicates heavier tails (more outliers) than what is expected in a normal distribution - leptokurtic\")\n",
    "print(\"→ Negative kurtosis indicates lighter tails (less outliers) than what is expected in a normal distribution - platykurtic\")\n",
    "print(\"→ A normal distribution has kurtosis of 0 - mesokurtic\")\n",
    "print(\"\\nKurtosis values:\")\n",
    "print(supply_chain_data[numeric_cols].apply(lambda x: kurtosis(x, fisher=True)))"
   ],
   "id": "a7c2dbd023e3fc36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Skewness of numeric columns",
   "id": "6eafac18529b96cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSkewness of numeric columns:\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"→ Positive skewness indicates a long right tail (right-skewed distribution)\")\n",
    "print(\"→ Negative skewness indicates a long left tail (left-skewed distribution)\")\n",
    "print(\"→ Skewness close to 0 indicates a symmetric distribution\")\n",
    "print(\"\\nSkewness values:\")\n",
    "print(supply_chain_data[numeric_cols].apply(lambda x: skew(x)))"
   ],
   "id": "32e3c3aa021f62cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Measures of Relationship",
   "id": "c90db9011e27cb7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Covariance matrix of numeric features",
   "id": "346349cb34019739"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nCovariance matrix of numeric features:\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"→ Positive values indicate that variables move in the same direction\")\n",
    "print(\"→ Negative values indicate that variables move in opposite directions\")\n",
    "print(\"→ Values close to 0 indicate little to no linear relationship\")\n",
    "print(\"\\nCovariance values:\")\n",
    "display(supply_chain_data[numeric_cols].cov())"
   ],
   "id": "407a8a4b1ac5a9c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation matrix of numeric features",
   "id": "191a10a5912c6c5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nSpearman's rank correlation matrix of numeric features:\")\n",
    "spearman_corr = supply_chain_data[numeric_cols].corr(method='spearman')\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"→ Values range from -1 to +1\")\n",
    "print(\"→ +1 indicates perfect positive correlation\")\n",
    "print(\"→ -1 indicates perfect negative correlation\")\n",
    "print(\"→ 0 indicates no correlation\")\n",
    "print(\"\\nCorrelation values:\")\n",
    "display(spearman_corr)"
   ],
   "id": "948555cb9fa5eab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Basic visualization of the data",
   "id": "fabfbd440a8d6dcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `n_cols = 3` Sets the number of plots per row to 3\n",
    "- `n_rows = (len(numeric_cols) // n_cols) + (1 if len(numeric_cols) % n_cols else 0)` Calculates the number of rows needed based on the number of numeric columns and the number of columns per row.\n",
    "- `plt.figure(figsize=(12, 5 * n_rows))` Sets the figure size to be wider and taller based on the number of rows.\n",
    "- `for i, col in enumerate(numeric_cols, 1):` Iterates over each numeric column (`numeric_cols`), starting the index at 1. `enumerate(numeric_cols, 1)` returns pairs of (index, value) for each item in the list. The 1 means that the index will start from 1, e.g., (1, 'Days for shipping (real)'), (2, 'Days for shipment (scheduled)'), etc.\n",
    "- `plt.subplot(n_rows, n_cols, i)` Creates a subplot in a grid layout with `n_rows` rows and `n_cols` columns, placing the current plot in the `i`-th position.\n",
    "- `sns.histplot(data=supply_chain_data, x=col)` Plots a histogram for the current numeric column using Seaborn's `histplot` function.\n",
    "- `sns.boxplot(data=supply_chain_data, y=col)` Plots a box plot for the current numeric column using Seaborn's `boxplot` function.\n",
    "- `sns.despine(right=True, top=True)` Removes the right and top spines (borders) of the plot for a cleaner look.\n",
    "- `plt.title(f'Distribution of {col}')` Sets the title of the current subplot to indicate which column's distribution is being shown.\n",
    "- `plt.grid(axis='y', alpha=0.2)` Adds a grid to the y-axis with a transparency level of 0.2 for better visibility.\n",
    "- `plt.grid(axis='x', visible=False)` Hides the grid for the x-axis to reduce clutter and increase the data-to-ink ratio.\n",
    "- `plt.tight_layout()` Adjusts the spacing between subplots to prevent overlap and ensure a clean layout.\n",
    "- `plt.show()` Displays the entire figure with all subplots."
   ],
   "id": "7c6836d3e020f2ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Histograms",
   "id": "8bd869e24bc0ba49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) // n_cols) + (1 if len(numeric_cols) % n_cols else 0)\n",
    "\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(data=supply_chain_data, x=col)\n",
    "    sns.despine(right=True, top=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.grid(axis='y', alpha=0.2)\n",
    "    plt.grid(axis='x', visible=False)\n",
    "plt.tight_layout()  # Adjust spacing\n",
    "plt.show()"
   ],
   "id": "faff92f1ddeee1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Box plots",
   "id": "461bc683e3ce022a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) // n_cols) + (1 if len(numeric_cols) % n_cols else 0)\n",
    "\n",
    "plt.figure(figsize=(15, 5 * n_rows))\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.boxplot(data=supply_chain_data, y=col)\n",
    "    sns.despine(right=True, top=True, bottom=True)\n",
    "    plt.title(f'Box Plot of {col}')\n",
    "    plt.grid(axis='y', alpha=0.2)\n",
    "    plt.grid(axis='x', visible=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d9186ffc9120f55e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Missing data plot",
   "id": "75c41d34067d306e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This visualization helps to quickly identify which columns have missing values and the extent of the missing data. The heatmap will show yellow for missing values and purple for non-missing values, making it easy to spot patterns of missingness. This is useful for understanding the completeness of the dataset and deciding how to handle missing values in subsequent analysis.\n",
    "- The code uses `sns.heatmap()` to visualize missing data in the DataFrame.\n",
    "- The code also uses the `isnull()` method to create a boolean DataFrame indicating where values are missing (True) or present (False).\n",
    "- `yticklabels=False` hides the y-axis labels to reduce clutter.\n",
    "- `cbar=False` removes the color bar, which is not necessary for this plot.\n",
    "- `cmap='viridis'` sets the color map to 'viridis' which is a perceptually uniform color map suitable for visualizing missing data.\n",
    "- `plt.title('Missing Data')` sets the title of the plot to 'Missing Data'\n",
    "- `plt.show()` displays the plot."
   ],
   "id": "b7b141338b0f8e7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(supply_chain_data.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "plt.title('Missing Data')\n",
    "plt.show()"
   ],
   "id": "59f8cef0024d9099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Correlation heatmap",
   "id": "20ed046a4b895b88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This visualization helps to quickly identify relationships between numeric features. The heatmap will show the strength and direction of correlations, with colors indicating positive (red) or negative (blue) correlations. This is useful for understanding how features relate to each other and can inform feature selection or feature engineering in subsequent analysis.\n",
    "- The code uses `sns.heatmap()` to visualize the Spearman correlation matrix of the numeric features in the DataFrame.\n",
    "- `annot=True` adds the correlation values as annotations in the heatmap.\n",
    "- `cmap='coolwarm'` sets the color map to 'coolwarm' which provides a gradient from blue (negative correlation) to red (positive correlation).\n",
    "- `center=0` centers the color map at 0, which is useful for visualizing both positive and negative correlations."
   ],
   "id": "46ba3282ad22980e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ],
   "id": "fa8aa2edcacd587a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Scatter plot matrix",
   "id": "21d07525a1bad44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This visualization helps to quickly identify relationships between pairs of numeric features. The scatter plot matrix will show scatter plots for each pair of numeric features, allowing for visual inspection of relationships, trends, and potential outliers. This is useful for understanding how features interact with each other and can inform feature selection or feature engineering in subsequent analysis.\n",
    "- The code uses `sns.pairplot()` to create a scatter plot matrix of the numeric features in the DataFrame"
   ],
   "id": "e7ece18ae3b8c524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.pairplot(supply_chain_data[numeric_cols])\n",
    "plt.suptitle('Scatter Plot Matrix', y=1.02)\n",
    "plt.show()"
   ],
   "id": "40306f538f995e64",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39ad8bcb",
   "metadata": {},
   "source": "## Step 4: Data preparation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove rows with missing target values (if any)",
   "id": "8d74822a3b4fd70d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This step is crucial because the target variable (`Late_delivery_risk`) is essential for training the kNN model. If there are missing values in the target variable, the model cannot learn from those observations, leading to potential errors during training and evaluation.\n",
    "- The code uses `dropna()` to remove rows where the target variable (`Late_delivery_risk`) is missing. This ensures that only complete cases are used for model training and evaluation.\n",
    "- `pd.get_dummies()` is used to convert the categorical variable `Shipping Mode` into dummy variables (one-hot encoding). This is necessary because Machine Learning algorithms typically require numerical input, and one-hot encoding transforms categorical variables into a format that can be provided to the ML algorithm.\n",
    "- `drop_first=True` is used to avoid the dummy variable trap, which occurs when one category can be perfectly predicted from the others. By dropping the first category, we reduce multicollinearity in the dataset."
   ],
   "id": "104b5f15ff88550"
  },
  {
   "cell_type": "code",
   "id": "ff3ad4b8",
   "metadata": {},
   "source": [
    "supply_chain_data = supply_chain_data.dropna(subset=['Late_delivery_risk'])\n",
    "supply_chain_data = pd.get_dummies(supply_chain_data, columns=['Shipping Mode'], drop_first=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create X and y datasets for the features and target variable respectively",
   "id": "84193463e037f13c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This step separates the features (independent variables) from the target variable (dependent variable) in the dataset. The features are stored in `X`, and the target variable is stored in `y`.\n",
    "- `X` contains all the columns except the target variable `Late_delivery_risk`, which is dropped using `drop()`.\n",
    "- `y` contains only the target variable `Late_delivery_risk`."
   ],
   "id": "638d09c48b307f45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = supply_chain_data.drop('Late_delivery_risk', axis=1)\n",
    "y = supply_chain_data['Late_delivery_risk']\n",
    "\n",
    "print(\"\\nThe number of observations and variables in the features dataset\")\n",
    "print(X.shape)\n",
    "\n",
    "print(\"\\nThe number of observations and variables in the target dataset\")\n",
    "print(y.shape)"
   ],
   "id": "9d7683ba580b9023",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40461cd2",
   "metadata": {},
   "source": [
    "### Train‑test split and scaling"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This step splits the dataset into training and testing sets to evaluate the model's performance on unseen data. The `train_test_split()` function is used to randomly split the data, ensuring that the target variable's distribution is preserved in both sets using `stratify=y`.\n",
    "- `test_size=0.3` indicates that 30% of the data will be used for testing, while 70% will be used for training.\n",
    "- `random_state=53` ensures reproducibility of the split, meaning that every time you run the code, you will get the same split of data.\n",
    "- `StandardScaler()` is used to standardize the features by setting mean = 0 and variance = 1. This is important for kNN, as it is sensitive to the scale of the features. Standardization ensures that all features contribute equally to the distance calculations.\n",
    "- `fit_transform()` is applied to the training data to compute the mean and standard deviation, and then transform the data accordingly.\n",
    "- `transform()` is applied to the test data using the same scaler fitted on the training data. This ensures that the test data is scaled in the same way as the training data, preventing data leakage."
   ],
   "id": "b040f0188718b135"
  },
  {
   "cell_type": "code",
   "id": "5464f9cc",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=53, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db641a49",
   "metadata": {},
   "source": "## Step 5: Create a baseline kNN model (k = 5)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This step creates a baseline kNN model using the `KNeighborsClassifier` from scikit-learn with `n_neighbors=5`. This means that the model will consider the 5 nearest neighbors to make predictions.\n",
    "- `fit()` is called on the training data to train the model, and `predict()` is used to make predictions on the test data.\n",
    "- `confusion_matrix()` is used to compute the confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "- `classification_report()` is used to generate a report that includes precision, recall, F1-score, and support for each class in the target variable. This provides a detailed evaluation of the model's performance.\n",
    "- This step is essential for establishing a baseline performance of the kNN model, which can be improved through hyperparameter tuning in subsequent steps."
   ],
   "id": "912aab2c796c558e"
  },
  {
   "cell_type": "code",
   "id": "eaca51ff",
   "metadata": {},
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88698f2f",
   "metadata": {},
   "source": "## Step 6: Perform hyperparameter tuning"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This step performs hyperparameter tuning using `GridSearchCV` to find the optimal number of neighbors (`n_neighbors`) and the weighting method (`weights`) for the kNN model.\n",
    "- `param_grid` defines the hyperparameters to be tuned:\n",
    "    - `n_neighbors`: A range of odd numbers from 3 to 15 (inclusive) to avoid ties in classification.\n",
    "    - `weights`: Two options: 'uniform' (all neighbors contribute equally) and 'distance' (closer neighbors contribute more).\n",
    "- `GridSearchCV` performs an exhaustive search over the specified hyperparameters using 5-fold cross-validation (`cv=5`). This means that the training data is split into 5 subsets, and the model is trained and evaluated 5 times, each time using a different subset as the validation set.\n",
    "- `n_jobs=-1` allows the search to use all available CPU cores for parallel processing, speeding up the tuning process.\n",
    "- `fit()` is called on the training data to perform the grid search, and the best hyperparameters are printed.\n",
    "- `best_score_` gives the best cross-validated accuracy score, and `best_params_` provides the optimal hyperparameters found during the search.\n",
    "- Finally, the optimal kNN model is evaluated on the test data using `score()`, which returns the accuracy of the model on the test set."
   ],
   "id": "91c1611107bdaa00"
  },
  {
   "cell_type": "code",
   "id": "719396e8",
   "metadata": {},
   "source": [
    "param_grid = {'n_neighbors': range(3, 17, 2),\n",
    "              'weights': ['uniform', 'distance']}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid,\n",
    "                    cv=5, n_jobs=-1)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print('Best params:', grid.best_params_)\n",
    "print('CV accuracy:', grid.best_score_.round(3))\n",
    "optimal_knn_model = grid.best_estimator_\n",
    "print('Test accuracy:', f\"{optimal_knn_model.score(X_test_scaled, y_test):.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 7: Make predictions on new data and save the results for reporting in Power BI",
   "id": "8cc4456d6edcba21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- This step demonstrates how to make predictions on new data using the optimal kNN model obtained from hyperparameter tuning.\n",
    "- The new data is loaded from a CSV file (`DataCoSupplyChainDataset_new_data.csv`), which should have the same structure as the training data.\n",
    "- The new data is preprocessed in the same way as the training data:\n",
    "    - Categorical variables are converted to dummy variables using `pd.get_dummies()`.\n",
    "    - Any missing columns that were present in the training data but not in the new data are added with a value of 0.\n",
    "    - The columns are reordered to match the training data.\n",
    "- The new data is scaled using the same `StandardScaler` fitted on the training data. Without this step, the model would not be able to make accurate predictions because the features would not be on the same scale as those used during training. kNN is sensitive to the scale of the features because it uses distance metrics (like Euclidean distance) to determine the nearest neighbors.\n",
    "- `predict()` is called on the scaled new data to get predictions for the `Late_delivery_risk` target variable.\n",
    "- The predictions are then added to the original new data DataFrame as a new column (`Predicted_Late_Delivery`).\n",
    "- The results are saved to a new CSV file (`DataCoSupplyChainDataset_predicted.csv`) for reporting in Power BI."
   ],
   "id": "632e3e05424a059d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load your new data\n",
    "\n",
    "# If you are using Google Colab, uncomment the following lines to mount your Google Drive and load the new data from there.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# new_data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/DataCoSupplyChainDataset_new_data.csv')\n",
    "\n",
    "new_data = pd.read_csv('./data/DataCoSupplyChainDataset_new_data.csv')\n",
    "\n",
    "# 2. Prepare the data (same preprocessing as training data)\n",
    "# Create fake variables for Shipping Mode\n",
    "new_data_processed = pd.get_dummies(new_data, columns=['Shipping Mode'], drop_first=True)\n",
    "\n",
    "# 3. Make sure you have all the necessary columns (match training data)\n",
    "missing_cols = set(X.columns) - set(new_data_processed.columns)\n",
    "for col in missing_cols:\n",
    "    new_data_processed[col] = 0\n",
    "\n",
    "# Ensure columns are in the same order as training data\n",
    "new_data_processed = new_data_processed[X.columns]\n",
    "\n",
    "# 4. Scale the data using the same scaler\n",
    "new_data_scaled = scaler.transform(new_data_processed)\n",
    "\n",
    "# 5. Make predictions\n",
    "predictions = optimal_knn_model.predict(new_data_scaled)\n",
    "\n",
    "# 6. Add predictions and probabilities of the predictions to the original dataframe and save it\n",
    "new_data['Predicted_Late_Delivery'] = predictions\n",
    "probabilities = optimal_knn_model.predict_proba(new_data_scaled)\n",
    "new_data['Late_Delivery_Probability_Class_0'] = probabilities[:, 0]  # Probability of on-time delivery\n",
    "new_data['Late_Delivery_Probability_Class_1'] = probabilities[:, 1]  # Probability of late delivery\n",
    "\n",
    "print(\"\\nThe new data with predictions:\")\n",
    "display(new_data)\n",
    "# If you are using Google Colab, uncomment the following lines to save the predicted data to your Google Drive.\n",
    "# new_data.to_csv('/content/drive/My Drive/Colab Notebooks/data/DataCoSupplyChainDataset_predicted_data.csv', index=False)\n",
    "new_data.to_csv('./data/DataCoSupplyChainDataset_predicted_data.csv', index=False)"
   ],
   "id": "1a85a45816abc1a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4a74ade0",
   "metadata": {},
   "source": [
    "## Step 8: Business insight\n",
    "A reliable early-warning model enables the supply chain management and the logistics teams to prioritize orders that are likely to be late, thus reducing penalty costs and improving customer satisfaction."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "Constante, F., Silva, F., & Pereira, A. (2019). DataCo Smart Supply Chain for Big Data Analysis (Version 5) [CSV]. Mendeley Data. https://doi.org/10.17632/8gx2fvg2k6.5"
   ],
   "id": "e8f5b0d9bb5fc8a3"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
